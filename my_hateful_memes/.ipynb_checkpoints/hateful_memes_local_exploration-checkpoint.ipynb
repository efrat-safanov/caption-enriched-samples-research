{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch                    \n",
    "import torchvision\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_bert import BertModel, BertForMaskedLM\n",
    "\n",
    "from mmf.utils.build import (\n",
    "    build_classifier_layer,\n",
    "    build_image_encoder,\n",
    "    build_text_encoder,\n",
    ")\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import os\n",
    "#from mmf.utils.configuration import ( load_yaml_with_defaults, get_zoo_config, get_config)\n",
    "import mmf.utils.configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_override=None, opts=['model=vilbert', 'config=../mmf/configs/datasets/hateful_memes/with_features.yaml', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original', 'dataset=hateful_memes', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/efrat/source/dl_course/mmf/mmf/utils/configuration.py:536: UserWarning: Device specified is 'cuda' but cuda is not present. Switching to CPU version.\n",
      "  \"Device specified is 'cuda' but cuda is not present. \"\n",
      "/Users/efrat/anaconda3/lib/python3.7/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': '/Users/efrat/.cache/torch/mmf/data/datasets', 'depth_first': False, 'fast_read': False, 'use_images': False, 'use_features': True, 'return_features_info': True, 'images': {'train': ['hateful_memes/defaults/images/'], 'val': ['hateful_memes/defaults/images/'], 'test': ['hateful_memes/defaults/images/']}, 'features': {'train': ['hateful_memes/defaults/features/detectron.lmdb'], 'val': ['hateful_memes/defaults/features/detectron.lmdb'], 'test': ['hateful_memes/defaults/features/detectron.lmdb']}, 'annotations': {'train': ['hateful_memes/defaults/annotations/train.jsonl'], 'val': ['hateful_memes/defaults/annotations/dev_seen.jsonl'], 'test': ['hateful_memes/defaults/annotations/test_seen.jsonl']}, 'max_features': 100, 'processors': {'text_processor': {'type': 'vocab', 'params': {'max_length': 14, 'vocab': {'type': 'intersected', 'embedding_name': 'glove.6B.300d', 'vocab_file': 'hateful_memes/defaults/extras/vocabs/vocabulary_100k.txt'}, 'preprocessor': {'type': 'simple_sentence', 'params': {}}}}, 'bbox_processor': {'type': 'bbox', 'params': {'max_length': 50}}, 'image_processor': {'type': 'torchvision_transforms', 'params': {'transforms': [{'type': 'Resize', 'params': {'size': [256, 256]}}, {'type': 'CenterCrop', 'params': {'size': [224, 224]}}, 'ToTensor', 'GrayScaleTo3Channels', {'type': 'Normalize', 'params': {'mean': [0.46777044, 0.44531429, 0.40661017], 'std': [0.12221994, 0.12145835, 0.14380469]}}]}}}}\n",
      "{'data_dir': '/Users/efrat/.cache/torch/mmf/data/datasets', 'depth_first': False, 'fast_read': False, 'use_images': False, 'use_features': True, 'return_features_info': True, 'images': {'train': ['hateful_memes/defaults/images/'], 'val': ['hateful_memes/defaults/images/'], 'test': ['hateful_memes/defaults/images/']}, 'features': {'train': ['hateful_memes/defaults/features/detectron.lmdb'], 'val': ['hateful_memes/defaults/features/detectron.lmdb'], 'test': ['hateful_memes/defaults/features/detectron.lmdb']}, 'annotations': {'train': ['hateful_memes/defaults/annotations/train.jsonl'], 'val': ['hateful_memes/defaults/annotations/dev_seen.jsonl'], 'test': ['hateful_memes/defaults/annotations/test_seen.jsonl']}, 'max_features': 100, 'processors': {'text_processor': {'type': 'bert_tokenizer', 'params': {'tokenizer_config': {'type': 'bert-base-uncased', 'params': {'do_lower_case': True}}, 'mask_probability': 0, 'max_seq_length': 128}}, 'transformer_bbox_processor': {'type': 'transformer_bbox', 'params': {'bbox_key': 'bbox', 'image_width_key': 'image_width', 'image_height_key': 'image_height'}}}}\n",
      "Sample([('text', ['[CLS]', 'its', 'their', 'character', 'not', 'their', 'color', 'that', 'matters', '[SEP]']), ('input_ids', tensor([ 101, 2049, 2037, 2839, 2025, 2037, 3609, 2008, 5609,  102,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])), ('input_mask', tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])), ('segment_ids', tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])), ('lm_label_ids', tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1])), ('tokens', ['[CLS]', 'its', 'their', 'character', 'not', 'their', 'color', 'that', 'matters', '[SEP]']), ('id', tensor(42953, dtype=torch.int32)), ('image_feature_0', tensor([[0.0000, 0.0000, 0.0000,  ..., 0.9350, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.8328, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [8.0939, 2.5830, 0.0000,  ..., 2.7080, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])), ('image_info_0', Sample([('feature_path', '42953'), ('image_height', 752), ('image_width', 512), ('num_boxes', 100), ('objects', array([ 391,  274,   50,  391,  119,  274,   50,  327,  467,  327,  391,\n",
      "        191,  467,  391,  452,   51,  467,   50,  546,  274,  191,  452,\n",
      "        467,  391,   50,  452,  750,  546,  391,  452,   50,  327,  191,\n",
      "        391,  452,  119,  274,   51, 1398,  274,  391,  467,  119,   50,\n",
      "        731,  327,  274,   51,  303,  467,  191,  452,  750, 1218,  452,\n",
      "        327,  438,  452,  327,   51,  452,  391,   50, 1398,   51,   51,\n",
      "        274,  303,  391,  467,  327,  303,  119, 1218, 1398,  438,   50,\n",
      "        248,  119,  391,  546,  327,  248,  119,  327,  191, 1218,  191,\n",
      "       1218,  388,  248,  303,  750,   50, 1218, 1398,  419,  546,   50,\n",
      "        391])), ('cls_prob', array([[2.76495769e-08, 3.01267078e-09, 1.02969135e-08, ...,\n",
      "        4.52948861e-08, 9.16549084e-07, 2.89735791e-08],\n",
      "       [8.75852265e-07, 8.11830489e-07, 1.00740863e-06, ...,\n",
      "        1.44937837e-06, 1.21444573e-05, 9.04431602e-07],\n",
      "       [1.85341928e-08, 4.99033490e-08, 1.66483278e-06, ...,\n",
      "        2.78158510e-08, 1.93905930e-06, 1.58411346e-08],\n",
      "       ...,\n",
      "       [4.58024587e-08, 5.55569502e-09, 7.48158584e-08, ...,\n",
      "        1.94629823e-08, 5.10909285e-06, 1.49401878e-08],\n",
      "       [4.61221816e-06, 9.47147601e-06, 3.39043036e-04, ...,\n",
      "        3.13494911e-06, 2.99764215e-05, 2.39203428e-06],\n",
      "       [6.49219434e-09, 3.10703752e-09, 1.45017518e-08, ...,\n",
      "        1.15668559e-08, 6.85656744e-07, 3.11828208e-09]], dtype=float32)), ('bbox', tensor([[3.7047e-01, 7.3074e-01, 4.4580e-01, 8.2928e-01, 7.4217e-03],\n",
      "        [6.9684e-01, 6.9117e-01, 8.0182e-01, 8.0816e-01, 1.2281e-02],\n",
      "        [1.6733e-01, 0.0000e+00, 7.2545e-01, 5.5199e-01, 3.0808e-01],\n",
      "        [3.4886e-01, 7.6306e-01, 4.6234e-01, 8.2788e-01, 7.3559e-03],\n",
      "        [3.5574e-01, 1.4733e-02, 6.7144e-01, 1.9086e-01, 5.5602e-02],\n",
      "        [3.5997e-01, 1.5783e-01, 4.3576e-01, 2.2416e-01, 5.0271e-03],\n",
      "        [3.8465e-01, 0.0000e+00, 9.1204e-01, 5.4372e-01, 2.8675e-01],\n",
      "        [4.6531e-01, 9.5356e-02, 7.7759e-01, 3.1436e-01, 6.8389e-02],\n",
      "        [5.0449e-01, 1.4009e-01, 5.7746e-01, 1.6067e-01, 1.5018e-03],\n",
      "        [3.6712e-01, 6.3920e-01, 6.7196e-01, 9.5529e-01, 9.6357e-02],\n",
      "        [5.9035e-01, 1.3056e-01, 6.6960e-01, 2.1035e-01, 6.3226e-03],\n",
      "        [4.4367e-01, 5.1747e-01, 9.9875e-01, 9.3336e-01, 2.3085e-01],\n",
      "        [4.9929e-01, 1.3069e-01, 5.9198e-01, 1.6435e-01, 3.1202e-03],\n",
      "        [3.4694e-01, 6.9625e-01, 4.4338e-01, 8.4604e-01, 1.4444e-02],\n",
      "        [3.8783e-01, 8.4208e-01, 4.6021e-01, 8.9364e-01, 3.7317e-03],\n",
      "        [3.4319e-01, 3.2037e-01, 7.2738e-01, 5.2951e-01, 8.0351e-02],\n",
      "        [4.2180e-01, 7.1886e-01, 5.2518e-01, 7.5841e-01, 4.0894e-03],\n",
      "        [6.3216e-02, 6.2262e-01, 9.9875e-01, 1.0068e+00, 3.5942e-01],\n",
      "        [5.1460e-01, 1.3063e-01, 6.9230e-01, 1.6290e-01, 5.7342e-03],\n",
      "        [7.1243e-01, 6.7536e-01, 8.3687e-01, 8.3232e-01, 1.9533e-02],\n",
      "        [3.0672e-01, 5.0270e-01, 7.7833e-01, 1.0068e+00, 2.3775e-01],\n",
      "        [5.5172e-01, 2.1713e-01, 6.8628e-01, 2.6525e-01, 6.4750e-03],\n",
      "        [4.4356e-01, 7.2800e-01, 5.0675e-01, 7.5205e-01, 1.5198e-03],\n",
      "        [5.7171e-01, 1.6332e-01, 6.7884e-01, 2.1067e-01, 5.0725e-03],\n",
      "        [5.7516e-03, 1.3084e-01, 9.9875e-01, 5.5024e-01, 4.1647e-01],\n",
      "        [3.9304e-01, 8.5009e-01, 4.6332e-01, 8.7568e-01, 1.7980e-03],\n",
      "        [4.0183e-01, 7.0431e-01, 5.2599e-01, 7.4406e-01, 4.9346e-03],\n",
      "        [4.9232e-01, 1.1906e-01, 7.1097e-01, 1.8229e-01, 1.3826e-02],\n",
      "        [3.0662e-01, 7.4068e-01, 4.6018e-01, 8.4944e-01, 1.6701e-02],\n",
      "        [5.7242e-01, 1.9454e-01, 7.1286e-01, 2.7334e-01, 1.1067e-02],\n",
      "        [2.3385e-01, 5.2550e-01, 6.6555e-01, 1.0068e+00, 2.0778e-01],\n",
      "        [4.2326e-01, 6.7739e-01, 8.2488e-01, 9.1940e-01, 9.7194e-02],\n",
      "        [3.2247e-01, 5.2264e-02, 6.6724e-01, 3.0172e-01, 8.6003e-02],\n",
      "        [3.6093e-01, 7.2109e-01, 5.1806e-01, 8.4832e-01, 1.9992e-02],\n",
      "        [6.0351e-01, 2.1007e-01, 6.9246e-01, 2.5998e-01, 4.4392e-03],\n",
      "        [3.2133e-01, 6.1882e-02, 7.2960e-01, 2.3422e-01, 7.0361e-02],\n",
      "        [3.6376e-01, 1.4146e-01, 4.5246e-01, 2.7334e-01, 1.1698e-02],\n",
      "        [9.9003e-02, 3.4778e-01, 7.8989e-01, 4.9289e-01, 1.0025e-01],\n",
      "        [5.6058e-01, 2.6449e-01, 6.8634e-01, 3.1165e-01, 5.9302e-03],\n",
      "        [6.0595e-01, 6.9097e-01, 8.2461e-01, 7.9597e-01, 2.2959e-02],\n",
      "        [5.5940e-01, 1.4251e-01, 6.8703e-01, 1.9400e-01, 6.5715e-03],\n",
      "        [6.4036e-01, 1.3722e-01, 6.9392e-01, 1.5770e-01, 1.0968e-03],\n",
      "        [3.5628e-01, 2.9721e-02, 6.2400e-01, 1.2414e-01, 2.5278e-02],\n",
      "        [5.3868e-02, 0.0000e+00, 9.7389e-01, 3.8214e-01, 3.5157e-01],\n",
      "        [4.5193e-01, 2.1142e-01, 6.9367e-01, 3.1521e-01, 2.5090e-02],\n",
      "        [3.2221e-01, 6.3844e-01, 5.2762e-01, 9.7356e-01, 6.8838e-02],\n",
      "        [7.0700e-01, 7.0820e-01, 7.8612e-01, 7.7427e-01, 5.2274e-03],\n",
      "        [0.0000e+00, 2.7389e-01, 6.7624e-01, 6.1544e-01, 2.3097e-01],\n",
      "        [4.1205e-01, 2.7903e-01, 6.5869e-01, 3.6386e-01, 2.0925e-02],\n",
      "        [5.2254e-01, 1.3766e-01, 5.9883e-01, 1.5759e-01, 1.5203e-03],\n",
      "        [2.8697e-01, 4.8992e-01, 9.2778e-01, 8.7144e-01, 2.4448e-01],\n",
      "        [3.9394e-01, 8.5875e-01, 4.5730e-01, 8.8679e-01, 1.7764e-03],\n",
      "        [3.8788e-01, 6.9392e-01, 5.1479e-01, 7.3144e-01, 4.7615e-03],\n",
      "        [5.8566e-01, 2.0516e-01, 6.9812e-01, 2.3983e-01, 3.8993e-03],\n",
      "        [5.7542e-01, 2.2330e-01, 6.9158e-01, 2.7000e-01, 5.4248e-03],\n",
      "        [3.9623e-01, 8.9313e-02, 6.8652e-01, 3.9797e-01, 8.9598e-02],\n",
      "        [4.1944e-01, 3.1162e-01, 6.6692e-01, 4.0962e-01, 2.4253e-02],\n",
      "        [3.8711e-01, 8.2619e-01, 4.8069e-01, 9.0880e-01, 7.7313e-03],\n",
      "        [5.0521e-01, 1.2484e-01, 6.8311e-01, 2.6649e-01, 2.5200e-02],\n",
      "        [1.1076e-01, 3.7902e-01, 8.3947e-01, 5.0874e-01, 9.4533e-02],\n",
      "        [5.8616e-01, 2.2994e-01, 7.0522e-01, 2.7483e-01, 5.3458e-03],\n",
      "        [5.4456e-01, 1.1675e-01, 6.8411e-01, 2.2174e-01, 1.4651e-02],\n",
      "        [4.4891e-01, 8.3865e-02, 7.5197e-01, 4.6243e-01, 1.1472e-01],\n",
      "        [4.2287e-01, 9.0125e-01, 5.5745e-01, 9.5379e-01, 7.0704e-03],\n",
      "        [1.5279e-01, 2.3250e-01, 9.9875e-01, 6.1028e-01, 3.1959e-01],\n",
      "        [6.3405e-03, 2.9846e-01, 9.1560e-01, 7.7678e-01, 4.3492e-01],\n",
      "        [6.3009e-01, 6.5391e-01, 8.4544e-01, 8.4435e-01, 4.1010e-02],\n",
      "        [6.4863e-01, 8.6043e-01, 8.4507e-01, 9.9492e-01, 2.6420e-02],\n",
      "        [3.5696e-01, 7.8223e-01, 4.3973e-01, 8.3766e-01, 4.5871e-03],\n",
      "        [6.2971e-01, 1.2656e-01, 7.0013e-01, 1.6663e-01, 2.8215e-03],\n",
      "        [3.9357e-01, 7.7267e-02, 7.6706e-01, 2.3340e-01, 5.8311e-02],\n",
      "        [6.7008e-01, 7.9766e-01, 9.9875e-01, 1.0068e+00, 6.8741e-02],\n",
      "        [4.3511e-01, 3.4669e-02, 7.1514e-01, 1.1466e-01, 2.2401e-02],\n",
      "        [3.8951e-01, 8.2601e-01, 4.7144e-01, 8.7606e-01, 4.1005e-03],\n",
      "        [4.1208e-01, 8.9289e-01, 4.8906e-01, 9.5006e-01, 4.4009e-03],\n",
      "        [2.1237e-01, 2.9838e-01, 7.2140e-01, 4.4699e-01, 7.5651e-02],\n",
      "        [0.0000e+00, 4.1833e-01, 7.1829e-01, 1.0068e+00, 4.2269e-01],\n",
      "        [6.9140e-03, 0.0000e+00, 5.6305e-01, 6.4497e-01, 3.5870e-01],\n",
      "        [3.8574e-01, 2.0714e-02, 7.0757e-01, 8.2815e-02, 1.9986e-02],\n",
      "        [3.7084e-01, 7.3495e-01, 4.8153e-01, 9.0533e-01, 1.8859e-02],\n",
      "        [5.1899e-01, 8.8141e-02, 6.9148e-01, 2.0454e-01, 2.0077e-02],\n",
      "        [4.9282e-01, 4.2654e-02, 7.5674e-01, 3.7394e-01, 8.7433e-02],\n",
      "        [4.7964e-02, 2.4209e-02, 3.2023e-01, 3.4353e-01, 8.6942e-02],\n",
      "        [4.5097e-01, 5.3426e-01, 8.4312e-01, 7.5764e-01, 8.7600e-02],\n",
      "        [4.1330e-01, 6.0081e-01, 7.9482e-01, 8.5512e-01, 9.7028e-02],\n",
      "        [3.9861e-01, 3.3582e-05, 6.8161e-01, 2.8329e-01, 8.0160e-02],\n",
      "        [3.8589e-01, 8.3804e-01, 4.4940e-01, 8.6747e-01, 1.8687e-03],\n",
      "        [4.3171e-01, 5.7503e-01, 8.1165e-01, 9.1359e-01, 1.2863e-01],\n",
      "        [5.1717e-01, 1.8856e-01, 7.1245e-01, 2.6769e-01, 1.5454e-02],\n",
      "        [5.3453e-01, 2.1426e-01, 6.9371e-01, 3.1068e-01, 1.5347e-02],\n",
      "        [5.0074e-01, 6.0885e-02, 9.9875e-01, 4.6392e-01, 2.0071e-01],\n",
      "        [3.7492e-01, 7.8006e-01, 9.9875e-01, 1.0068e+00, 1.4145e-01],\n",
      "        [4.2020e-01, 7.0307e-01, 5.1089e-01, 7.2298e-01, 1.8058e-03],\n",
      "        [2.4921e-01, 6.8449e-01, 9.7840e-01, 1.0068e+00, 2.3504e-01],\n",
      "        [3.7911e-01, 8.2763e-01, 4.5599e-01, 8.7319e-01, 3.5037e-03],\n",
      "        [4.0129e-01, 8.7966e-01, 5.4017e-01, 9.7108e-01, 1.2697e-02],\n",
      "        [4.3428e-01, 6.0923e-02, 7.1707e-01, 1.4164e-01, 2.2827e-02],\n",
      "        [5.4014e-01, 1.4180e-01, 6.8834e-01, 1.5443e-01, 1.8726e-03],\n",
      "        [1.2263e-01, 4.8337e-01, 9.9875e-01, 8.5441e-01, 3.2508e-01],\n",
      "        [5.6910e-01, 1.4243e-01, 7.1630e-01, 2.4511e-01, 1.5113e-02]])), ('max_features', tensor(100))])), ('targets', tensor(0))])\n"
     ]
    }
   ],
   "source": [
    "#create dataset for vilbert\n",
    "#future note - this oads the dataset as a global variable so no further changes can be made\n",
    "\n",
    "\n",
    "from mmf.common.registry import registry\n",
    "from mmf.utils.build import build_dataset\n",
    "from mmf.utils.configuration import Configuration\n",
    "from mmf.utils.configuration import load_yaml\n",
    "import argparse\n",
    "from mmf.utils.flags import flags\n",
    "parser = flags.get_parser()\n",
    "\n",
    "from mmf.utils.env import setup_imports\n",
    "setup_imports()\n",
    "args_str = \"\"\n",
    "\n",
    "opts = [\"model=vilbert\", \"config=../mmf/configs/datasets/hateful_memes/with_features.yaml\", \"run_type=val\",\n",
    "        \"checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original\",\n",
    "        \"dataset=hateful_memes\", \"dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl\", \"dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl\"]\n",
    "args = argparse.Namespace(config_override=None)\n",
    "args.opts = opts\n",
    "print(args)\n",
    "configuration = Configuration(args)\n",
    "dataset_config = configuration.get_config()\n",
    "\n",
    "model_data_processing_config = load_yaml(\"../mmf/projects/hateful_memes/configs/vilbert/defaults.yaml\")\n",
    "dataset_config.dataset_config.hateful_memes.use_images = False\n",
    "print(dataset_config.dataset_config.hateful_memes)\n",
    "\n",
    "#replace origina dataset processors with vilbert processors\n",
    "dataset_config.dataset_config.hateful_memes.processors = model_data_processing_config.dataset_config.hateful_memes.processors\n",
    "\n",
    "print(dataset_config.dataset_config.hateful_memes)\n",
    "dataset = build_dataset(\"hateful_memes\", dataset_config.dataset_config.hateful_memes)\n",
    "\n",
    "#sanity\n",
    "print(dataset.__getitem__(0))\n",
    "\n",
    "#for visualization - relevant for images loading\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "#dataset.visualize(num_samples=8, size=(512, 512), nrow=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/efrat/source/dl_course/my_hateful_memes\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dde117e5d39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"test_seen.jsonl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_samples_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtrain_samples_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             obj = self._get_object_parser(\n\u001b[0;32m--> 534\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m             )\n\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 871\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             decoded = {str(k): v for k, v in compat.iteritems(\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(Path.cwd())\n",
    "data_dir = Path(\"/home/efrat/.cache/torch/mmf/data/datasets/hateful_memes/defaults\")\n",
    "annotations_dir = data_dir / \"annotations\" \n",
    "\n",
    "\n",
    "img_path = data_dir / \"images\" \n",
    "train_path = annotations_dir / \"train.jsonl\"\n",
    "dev_path = annotations_dir / \"dev_seen.jsonl\"\n",
    "test_path = annotations_dir / \"test_seen.jsonl\"\n",
    "\n",
    "train_samples_frame = pd.read_json(train_path, lines=True)\n",
    "train_samples_frame.head()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "images = [\n",
    "    Image.open(\n",
    "        img_path / train_samples_frame.loc[i, \"img\"]\n",
    "    ).convert(\"RGB\")\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "for image in images:\n",
    "    print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "index_of_data = 0\n",
    "plt.imshow(images[index_of_data])\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(train_samples_frame.loc[index_of_data, \"text\"])\n",
    "#model.classify(images[7],train_samples_frame.loc[7, \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'vilbert_with_tokens.ViLBERTWithTokens'>\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from mmf.utils.build import (\n",
    "    build_model, build_processors\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from mmf.models.vilbert import ViLBERTBase\n",
    "from vilbert_with_tokens import ViLBERTWithTokens\n",
    "from transformers import BertConfig\n",
    "from omegaconf import OmegaConf\n",
    "#from mmf.models.vilbert import ViLBERTForClassification\n",
    "\n",
    "\n",
    "#model = MMBT.from_pretrained(\"mmbt.hateful_memes.images\")\n",
    "\n",
    "model_cls = registry.get_model_class(\"vilbert\")\n",
    "print(model_cls)\n",
    "model = model_cls.from_pretrained(\"vilbert.finetuned.hateful_memes.from_cc_original\")\n",
    "#model = model_cls.from_pretrained(\"vilbert.pretrained.cc.original\")\n",
    "model.build()\n",
    "\n",
    "#vilbert_config = load_yaml(\"../mmf/mmf/configs/models/vilbert/defaults.yaml\")\n",
    "#print(\"**********************\")\n",
    "#print(vilbert_config)\n",
    "#vilbert_config.model_config.vilbert.training_head_type = \"classification\"\n",
    "#print(\"*************************\")\n",
    "#model = ViLBERTBase.from_pretrained(vilbert_config.model_config.vilbert.bert_model_name, \\\n",
    "#                                        config=BertConfig.from_dict(OmegaConf.to_container(vilbert_config.model_config.vilbert, resolve=True)))\n",
    "#mmf_model_csl = registry.get_model_class(\"mmf_transformer\")\n",
    "#mmf_model = mmf_model_csl.from_pretrained(\"mmf_transformer\")\n",
    "#mmf_model.build()\n",
    "\n",
    "#processor_dict = build_processors(config.dataset_config.hateful_memes.processors)\n",
    "\n",
    "#model = ViLBERTWithTokens.from_pretrained(\"vilbert.finetuned.hateful_memes.direct\")\n",
    "#config = get_config(\"../mmf/projects/vilbert/configs/hateful_memes/defaults.yaml\")\n",
    "#model = build_model(config.model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainer': 'mmf', 'seed': -1, 'experiment_name': 'run', 'max_updates': 22000, 'max_epochs': None, 'log_interval': 100, 'logger_level': 'info', 'log_format': 'simple', 'log_detailed_config': False, 'should_not_log': False, 'colored_logs': True, 'tensorboard': False, 'batch_size': 512, 'update_frequency': 1, 'num_workers': 4, 'fast_read': False, 'dataset_size_proportional_sampling': True, 'pin_memory': False, 'checkpoint_interval': 1000, 'evaluation_interval': 1000, 'clip_gradients': False, 'clip_norm_mode': 'all', 'early_stop': {'enabled': False, 'patience': 4000, 'criteria': 'total_loss', 'minimize': True}, 'lr_scheduler': False, 'lr_steps': [], 'lr_ratio': 0.1, 'use_warmup': False, 'warmup_factor': 0.2, 'warmup_iterations': 1000, 'device': 'cpu', 'local_rank': None, 'verbose_dump': False, 'find_unused_parameters': False, 'evaluate_metrics': False, 'detect_anomaly': False, 'fp16': False}\n",
      "103\n",
      "102\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "from transformers.tokenization_auto import AutoTokenizer\n",
    "\n",
    "from mmf.utils.build import (\n",
    "    build_dataloader_and_sampler\n",
    ")\n",
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    model = model.cuda(0)\n",
    "\n",
    "\n",
    "#config = get_config(\"../mmf/projects/vilbert/configs/hateful_memes/defaults.yaml\")\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "bert_model_version = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_version)\n",
    "print(dataset_config.training)\n",
    "dataset_config.training.num_workers = 0\n",
    "dataset_config.training.batch_size = 1\n",
    "dataloader, sampler = build_dataloader_and_sampler(dataset, dataset_config.training)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent)+[0]*(128-len(sent)) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]\n",
    "print(mask_id)\n",
    "print(sep_id)\n",
    "print(cls_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=0, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
    "    elif sample:        \n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1)\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1)\n",
    "    print(idx.tolist())\n",
    "    return idx.tolist() if return_list else idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation modes as functions\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
    "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
    "    #if rand_init:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
    "    \n",
    "    return tokenize_batch(batch)\n",
    "\n",
    "def parallel_sequential_generation(original_batch, seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    \n",
    "    print(\"batch_size: \") \n",
    "    prepared_batch = dataset.prepare_batch(original_batch)\n",
    "    print(prepared_batch.get_batch_size())\n",
    "    batch_size = prepared_batch.get_batch_size()\n",
    "    print(prepared_batch.fields())\n",
    "    init_text_ids = get_init_text(seed_text, max_len, batch_size)\n",
    "    print(init_text_ids)\n",
    "    print(\"text_before:\")\n",
    "    print(prepared_batch.get_field(\"text\"))\n",
    "    print(\"input ids before:\")\n",
    "    print(prepared_batch.get_field(\"input_ids\"))\n",
    "\n",
    "   # check = dataset.__getitem__(0)          \n",
    "    text_inp = torch.tensor(init_text_ids)\n",
    "    prepared_batch.add_field(\"input_ids\", text_inp)\n",
    "\n",
    "\n",
    "    for ii in range(max_iter):\n",
    "        kk = np.random.randint(0, max_len)\n",
    "        for jj in range(batch_size):\n",
    "            prepared_batch.get_field(\"input_ids\")[jj][seed_len+kk] = mask_id\n",
    "\n",
    "        print(\"input ids after:\")\n",
    "        print(prepared_batch.get_field(\"input_ids\")) \n",
    "        print(\"-----------------------------------------------\")\n",
    "        out = model(prepared_batch)\n",
    "        print(model.children())\n",
    "        state = model.state_dict\n",
    "        t_pooler_layer = state()['model.bert.t_pooler.dense.weight']\n",
    "\n",
    "    #    print(out)\n",
    "        print(t_pooler_layer)\n",
    "     #   print(\"now getting image captioning only\")\n",
    "        topk = top_k if (ii >= burnin) else 0\n",
    "        idxs = generate_step(t_pooler_layer, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
    "        #print(idxs.type())\n",
    "        input_ids_batch = prepared_batch.get_field(\"input_ids\").numpy()\n",
    "        print(input_ids_batch)\n",
    "\n",
    "        for jj in range(batch_size):\n",
    "            input_ids_batch[jj][seed_len+kk] = idxs#idxs[jj]\n",
    "\n",
    "        prepared_batch.add_field(\"input_ids\", torch.from_numpy(input_ids_batch))\n",
    "\n",
    "        if verbose and np.mod(ii+1, print_every) == 0:\n",
    "            for_print = tokenizer.convert_ids_to_tokens(prepared_batch.get_field(\"input_ids\").numpy()[0])\n",
    "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "            print(\"iter\", ii+1, \" \".join(for_print))\n",
    "    return untokenize_batch(prepared_batch.get_field(\"input_ids\"))    \n",
    "\n",
    "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
    "                        cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for all positions at a time step \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        for kk in range(max_len):\n",
    "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
    "            for jj in range(batch_size):\n",
    "                batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii, print_every) == 0:\n",
    "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
    "    \n",
    "    return untokenize_batch(batch)\n",
    "            \n",
    "def sequential_generation(original_batch, seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
    "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
    "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    batch = batch.cuda() if cuda else batch\n",
    "    \n",
    "    prepared_batch = dataset.prepare_batch(original_batch)\n",
    "    print(prepared_batch.get_batch_size())\n",
    "    batch_size = prepared_batch.get_batch_size()\n",
    "    init_text_ids = get_init_text(seed_text, max_len, batch_size)\n",
    "    print(init_text_ids)\n",
    "    print(\"text_before:\")\n",
    "    print(prepared_batch.get_field(\"text\"))\n",
    "    print(\"input ids before:\")\n",
    "    print(prepared_batch.get_field(\"input_ids\"))\n",
    "    text_inp = torch.tensor(init_text_ids)\n",
    "    prepared_batch.add_field(\"input_ids\", text_inp)\n",
    "    print(\"input ids after:\")\n",
    "    print(prepared_batch.get_field(\"input_ids\")) \n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "\n",
    "    for ii in range(max_len):\n",
    "        before = prepared_batch.get_field(\"input_ids\")\n",
    "        before[0][seed_len+ii+leed_out_len] = sep_id\n",
    "       #does it work automatically? \n",
    "      #  inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
    "      #  iprepared_batch.get_field(\"input_ids\")np = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        print(prepared_batch.get_field(\"input_ids\"))\n",
    "        #new_model = nn.Sequential(*list(model.children())[:-1])\n",
    "        out = model(prepared_batch)\n",
    "        print(out)\n",
    "        state = model.state_dict\n",
    "        print(state().keys())\n",
    "        #print(model.c_layer5)\n",
    "        for name, module in model.named_children():\n",
    "            print(name)\n",
    "        #t_pooler_layer = state()['model.bert.t_pooler.dense.weight'] + state()[\"model.bert.t_pooler.dense.bias\"]\n",
    "        t_pooler_layer = state()[\"model.classifier.0.dense.weight\"] + state()[\"model.classifier.0.dense.bias\"]\n",
    "        #maybe register forward hook to get the output???\n",
    "        idxs = generate_step(t_pooler_layer, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
    "        for jj in range(batch_size):\n",
    "            prepared_batch.get_field(\"input_ids\")[jj][seed_len+ii] = idxs\n",
    "\n",
    "    return untokenize_batch(prepared_batch.get_field(\"input_ids\"))\n",
    "\n",
    "\n",
    "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
    "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
    "             cuda=False, print_every=1):\n",
    "    # main generation function to call\n",
    "    sentences = []\n",
    "    start_time = time.time()\n",
    "    batch_n = 0\n",
    "    for original_batch in tqdm.tqdm(dataloader):\n",
    "        #batch = parallel_sequential_generation(original_batch, seed_text, max_len=max_len, top_k=top_k,\n",
    "        #                                       temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
    "        #                                       cuda=cuda, verbose=True)\n",
    "        \n",
    "        batch = sequential_generation(original_batch, seed_text, batch_size=1, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\n",
    "        #batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\n",
    "        \n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "        batch_n += 1\n",
    "        sentences += batch\n",
    "        print(batch)\n",
    "        if batch_n > 2:\n",
    "            break\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)[1:-1]\n",
    "    print(\" \".join(sent))\n",
    "    \n",
    "def read_sents(in_file, should_detokenize=False):\n",
    "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
    "    if should_detokenize:\n",
    "        sents = [detokenize(sent) for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def write_sents(out_file, sents, should_detokenize=False):\n",
    "    with open(out_file, \"w\") as out_fh:\n",
    "        for sent in sents:\n",
    "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
    "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "device must be either 'str' or 'torch.device' type, <class 'NoneType'> found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-91a9a6bff158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n\u001b[1;32m     20\u001b[0m                               \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                               cuda=False)\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mwrite_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_detokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-02fb56441f27>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(n_samples, seed_text, batch_size, max_len, sample, top_k, temperature, burnin, max_iter, cuda, print_every)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m#                                       cuda=cuda, verbose=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequential_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleed_out_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleed_out_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;31m#batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-02fb56441f27>\u001b[0m in \u001b[0;36msequential_generation\u001b[0;34m(original_batch, seed_text, batch_size, max_len, leed_out_len, top_k, temperature, sample, cuda)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mprepared_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepared_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/dl_course/mmf/mmf/datasets/concat_dataset.py\u001b[0m in \u001b[0;36m_call_all_datasets_func\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_all_datasets_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# TODO: Log a warning here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/dl_course/mmf/mmf/datasets/base_dataset.py\u001b[0m in \u001b[0;36mprepare_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Try converting to SampleList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSampleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/dl_course/mmf/mmf/common/sample.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 raise TypeError(\n\u001b[1;32m    342\u001b[0m                     \u001b[0;34m\"device must be either 'str' or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                     \u001b[0;34m\"'torch.device' type, {} found\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m                 )\n\u001b[1;32m    345\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: device must be either 'str' or 'torch.device' type, <class 'NoneType'> found"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "batch_size = 50\n",
    "max_len = 40\n",
    "top_k = 100\n",
    "temperature = 0.7\n",
    "\n",
    "leed_out_len = 5 # max_len\n",
    "burnin = 250\n",
    "sample = True\n",
    "max_iter = 500\n",
    "registry.register(\"current_device\", torch.device('cuda'))\n",
    "\n",
    "# Choose the prefix context\n",
    "seed_text = \"[CLS]\".split()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for temp in [1.0]:\n",
    "        model_version = \"v1\"\n",
    "        bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
    "                              sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
    "                              cuda=False)\n",
    "        out_file = \"data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
    "        write_sents(out_file, bert_sents, should_detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(tokenizer.vocab.keys())[5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
